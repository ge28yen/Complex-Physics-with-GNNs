{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ge28yen/Complex-Physics-with-GNNs/blob/main/Copy_of_GNN_Physics_my_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Import dependencies, set up configuration\n",
        "\n"
      ],
      "metadata": {
        "id": "20ayZktr3kIV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7B5RONqz6FR",
        "outputId": "01ce3437-1895-4f27-f23a-5afa5c3afc22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.10/dist-packages (1.6.3+pt25cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-cluster) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-cluster) (1.26.4)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.10/dist-packages (2.1.2+pt25cu121)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.10/dist-packages (0.6.18+pt25cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.26.4)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.11.10)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch-geometric) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from typing import *\n",
        "import logging\n",
        "from typing import *\n",
        "import numpy as np\n",
        "import math\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Install torch geometric\n",
        "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
        "!pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch_geometric as pyg\n",
        "import torch_scatter\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # WARNING: the dataset is currenlty only available on the"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6PWvgQB3_H4",
        "outputId": "d6d00f58-98a8-49e3-8922-021739241a57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYvqOT_FVEL2"
      },
      "outputs": [],
      "source": [
        "# Configure the logging level and format\n",
        "logging.basicConfig(\n",
        "    level=logging.ERROR,  #set to logging.INFO if you want the debugging messages shown, logging.ERROR otherwise\n",
        "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
        "    force=True\n",
        ")\n",
        "\n",
        "METADATA = {\n",
        "    'epochs': 1,\n",
        "    'learning_rate': 0.005,\n",
        "    'batch_size': 4, #even this batch size overloads the available GPU sometimes\n",
        "    'connectivity_radius': 0.02,\n",
        "    'borders_x' : [0.1, 0.9],\n",
        "    'borders_y' : [0.1, 0.9],\n",
        "    'timestep' : 1, #time lag duration\n",
        "    'embedding_dimension': 16,\n",
        "    'hidden_dimension': 128\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzIEfYHE6-ID",
        "outputId": "7dfb32fa-5ab9-4eff-e1df-0ed4ffda4de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "wandb.login(key=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Define the Dataset"
      ],
      "metadata": {
        "id": "-IGinS5WC5ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "The data in the form of needs to be preprocessed. The input graph to to the model should have the following attributes:\n",
        "- graph.x : an array of ints indicating particle types\n",
        "- graph.y : an array of n_dim - dimensional"
      ],
      "metadata": {
        "id": "V-Rzx6dr9s-I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q0rG0hkdkbAK"
      },
      "outputs": [],
      "source": [
        "def velocities_from_positions(positions_seq, timestep):\n",
        "    #Calculation: velocity_i = position_i - position_i-1 / timestep\n",
        "\n",
        "    velocities = positions_seq[:, 1:, :] - positions_seq [:, :-1, :]\n",
        "    logging.info(f'velocities shape: {velocities.shape}')\n",
        "\n",
        "    return velocities\n",
        "\n",
        "def recalculate_positions(target_positions, x_boundaries, y_boundaries):\n",
        "    #Calculate distances to boundaries from particle positions\n",
        "\n",
        "    x_target_positions = target_positions[:, 0, 0].squeeze()\n",
        "    y_target_positions = target_positions[:, 0, 1].squeeze()\n",
        "    x_boundaries = torch.stack((x_target_positions-x_boundaries[0], x_boundaries[1]-x_target_positions), dim=1) # shape = (400, 2),\n",
        "    y_boundaries = torch.stack((y_target_positions-y_boundaries[0], y_boundaries[1]-y_target_positions), dim=1)\n",
        "    logging.info(f'x_boundaries shape: {x_boundaries.shape}')\n",
        "    logging.info(f'y_boundaries shape: {y_boundaries.shape}')\n",
        "\n",
        "    return x_boundaries, y_boundaries\n",
        "\n",
        "def acceleration_from_velocities(velocities, timestep):\n",
        "    #Calculate ast acceleration from velocities\n",
        "\n",
        "    acceleration = (velocities[:, 1:] - velocities[:, :-1])/timestep\n",
        "    logging.info(f'acceleration shape, {acceleration.shape}')\n",
        "    last_acceleration = acceleration[:,-1] # TODO: this can be done more efficient\n",
        "    logging.info(f'last acceleration, {last_acceleration[:5]}')\n",
        "\n",
        "    return last_acceleration\n",
        "    # Calculate accelerations from velocities\n",
        "\n",
        "def get_edge_features(edge_indexes, target_positions):\n",
        "    #Calculate relative displacements and absolute distance of the edges\n",
        "\n",
        "    transposed_edge_indexes = torch.t(edge_indexes)\n",
        "    edge_features = []\n",
        "    for two_nodes in transposed_edge_indexes.numpy():\n",
        "      node1 = two_nodes[0]\n",
        "      node2 = two_nodes[1]\n",
        "      position_node1 = target_positions.squeeze().numpy()[node1]\n",
        "      position_node2 = target_positions.squeeze().numpy()[node2]\n",
        "      relative_position = [position_node1[0] - position_node1[0], position_node2[1] - position_node1[0]]\n",
        "      distance = math.sqrt(sum(x**2 for x in relative_position))\n",
        "      relative_position.append(distance)\n",
        "      edge_features.append(relative_position)\n",
        "    edge_features = torch.tensor(edge_features)\n",
        "    logging.info(f'edge_features_shape, {edge_features.shape}')\n",
        "\n",
        "    return edge_features\n",
        "\n",
        "def preprocess(particle_type, positions_seq ,metadata): # tensors of shape (n_particles), (n_particles, n_timesteps, dim),\n",
        "\n",
        "  #0. preprocess the postion_sequence:\n",
        "  target_positions = positions_seq[:, -1: :]\n",
        "  previous_positions = positions_seq[:, :-1]\n",
        "\n",
        "  #1.Calculate velocities from previous_positons\n",
        "  timestep = metadata['timestep']\n",
        "  velocities = velocities_from_positions(positions_seq, timestep)\n",
        "\n",
        "  #2.Recalculate positons as given boundaries\n",
        "  borders_x = metadata['borders_x']\n",
        "  borders_y = metadata['borders_y']\n",
        "  x_boundaries, y_boundaries  = recalculate_positions(borders_x, borders_y)\n",
        "\n",
        "  #3. Calculate the edge indexes:\n",
        "  target_positions.squeeze()\n",
        "  edge_indexes = pyg.nn.radius_graph(target_positions.squeeze(), metadata['connectivity_radius'])\n",
        "  logging.info('edge_indexes shape, {edge_indexes.shape}')       # Should have shape (2, n_edges)\n",
        "\n",
        "\n",
        "  #4. Calculate the edge features:\n",
        "  edge_features = get_edge_features(edge_indexes, target_positions)\n",
        "\n",
        "  #5. Calculate the accelerations:\n",
        "  acceleration = acceleration_from_velocities(velocities, timestep)\n",
        "\n",
        "  ## 5. Sum it all up in a graph:\n",
        "  flattened_velocities = velocities.view(velocities.shape[0], -1)\n",
        "\n",
        "  graph = pyg.data.Data(\n",
        "      x = particle_type,\n",
        "      edge_index = edge_indexes,\n",
        "      node_features =torch.cat((x_boundaries, y_boundaries, flattened_velocities), dim =-1 ),#torch.cat(None, dim = -1)\n",
        "      edge_features =edge_features,\n",
        "      y = acceleration\n",
        "  )\n",
        "\n",
        "  return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the Datasets"
      ],
      "metadata": {
        "id": "la2L9YxlDPRh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "guLyZN4lODU6"
      },
      "outputs": [],
      "source": [
        "|#check the file loading\n",
        "base_path = '/content/drive/MyDrive/GGN_for_physics_DATA/'\n",
        "valid_pth_path = base_path +\"/valid_dataset.pth\"\n",
        "valid_json_path = base_path +\"/valid_offsets.json\"\n",
        "test_pth_path = base_path + \"/test_dataset.pth\"\n",
        "test_json_path = base_path + \"/test_offsets.json\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "P3cPNr89zuvM"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "class ShortDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, pth_path, offsets_path):\n",
        "        super().__init__()\n",
        "        self.dataset = torch.load(pth_path)\n",
        "        with open(offsets_path, 'rb') as f:\n",
        "          self.offsets = json.load(f)\n",
        "\n",
        "        logging.info(f'self.offsets, {self.offsets}')\n",
        "        self.length = int(list(self.offsets.keys())[-1]) #the length will be the number of the last\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        offset_id = self.offsets[str(idx)]\n",
        "        particle_type_offset = offset_id['particle_type']['offset']\n",
        "        position_offset = offset_id['position']['offset']\n",
        "\n",
        "        shape =offset_id['position']['shape']\n",
        "        n_particles = shape[0]\n",
        "        positions = self.dataset['position'][position_offset:position_offset+n_particles]\n",
        "        particle_types = self.dataset['particle_type'][particle_type_offset:particle_type_offset +n_particles]\n",
        "        graph  = preprocess(particle_types, positions, METADATA)\n",
        "        return graph\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "Z9W39tK_RX_Q",
        "outputId": "9dbb2b3b-7d68-4730-fe30-4a0a4deb55e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-07d270df6170>:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  self.dataset = torch.load(pth_path)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29849\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'METADATA' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5adc45a40777>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_short_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_short_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1278\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-07d270df6170>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpositions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'position'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition_offset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mposition_offset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mparticle_types\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'particle_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparticle_type_offset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mparticle_type_offset\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mn_particles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgraph\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparticle_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMETADATA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'METADATA' is not defined"
          ]
        }
      ],
      "source": [
        "#test the Short Dataset\n",
        "valid_short_dataset = ShortDataset(valid_pth_path, valid_json_path)\n",
        "test_short_dataset = ShortDataset(test_pth_path, test_json_path)\n",
        "\n",
        "print(len(valid_short_dataset))\n",
        "graph = test_short_dataset[1278]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lh-a7AAZLTp"
      },
      "source": [
        "## Visualize the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETPbiB1SL8YF"
      },
      "outputs": [],
      "source": [
        "# Visualize a datapoint:\n",
        "if False:\n",
        "  import numpy as np\n",
        "  import matplotlib.pyplot as plt\n",
        "  from matplotlib.animation import FuncAnimation\n",
        "  from matplotlib import animation\n",
        "  from IPython.display import HTML\n",
        "\n",
        "  new_positions = position.transpose(0,1)\n",
        "\n",
        "  # Create example data: Random walk with (timesteps, n, 2)\n",
        "  n = 100  # Number of points\n",
        "  timesteps = 50  # Number of frames\n",
        "  data = np.cumsum(np.random.randn(timesteps, n, 2), axis=0)  # Random walk data\n",
        "\n",
        "  data = new_positions.numpy()\n",
        "  # Set up the figure and axis\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.set_xlim(np.min(data[:, :, 0]), np.max(data[:, :, 0]))\n",
        "  ax.set_ylim(np.min(data[:, :, 1]), np.max(data[:, :, 1]))\n",
        "  sc = ax.scatter([], [], s=10)\n",
        "\n",
        "  # Initialization function\n",
        "  def init():\n",
        "      sc.set_offsets(np.empty((0, 2)))  # Empty 2D array for initialization\n",
        "      return sc,\n",
        "\n",
        "  # Update function\n",
        "  def update(frame):\n",
        "      offsets = data[frame]  # Extract frame data of shape (n, 2)\n",
        "      sc.set_offsets(offsets)\n",
        "      return sc,\n",
        "\n",
        "  # Create the animation\n",
        "  ani = FuncAnimation(\n",
        "      fig, update, frames=6, init_func=init, blit=True, interval=200\n",
        "  )\n",
        "\n",
        "  HTML(ani.to_html5_video())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JswY6B2lJep-"
      },
      "outputs": [],
      "source": [
        "# Implement this later\n",
        "class LongDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, pth_path, json_path):\n",
        "    super().__init__()\n",
        "    with open(pth_path, 'rb') as f:\n",
        "      self.dataset = f\n",
        "    with open(json_path) as f:\n",
        "      self.offsets = json.load(f)\n",
        "  def len(self):\n",
        "    return None\n",
        "  def get(self, idx):\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tTJNjwQBUH2"
      },
      "source": [
        "# 2. Define the GNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAVsGOCtBXOq"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
        "    assert n_layers>=2\n",
        "    super().__init__()\n",
        "    self.MLP=nn.ModuleList()\n",
        "    self.MLP.append(nn.Linear(input_size, hidden_size))\n",
        "    self.MLP.append(nn.ReLU())\n",
        "    for i in range(1, n_layers-1):\n",
        "      if i == n_layers-2:\n",
        "        self.MLP.append(nn.Linear(hidden_size, output_size))\n",
        "      else:\n",
        "        self.MLP.append(nn.Linear(hidden_size, hidden_size))\n",
        "        self.MLP.append(nn.ReLU())\n",
        "\n",
        "    # idk what this reset_parameters is\n",
        "    # they also do LayerNorm\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.MLP:\n",
        "      x= layer(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72fOYM20VJyE"
      },
      "outputs": [],
      "source": [
        "class Processor(pyg.nn.MessagePassing):\n",
        "  def __init__(self, hidden_size, n_layers):\n",
        "    super().__init__()\n",
        "    self.lin_node = MLP(hidden_size*2, hidden_size, hidden_size, n_layers)\n",
        "    self.lin_edge = MLP(hidden_size*3, hidden_size, hidden_size, n_layers)\n",
        "\n",
        "  def forward(self, x, edge_index, edge_feature):\n",
        "      edge_out, aggr = self.propagate(edge_index, x=(x, x), edge_feature=edge_feature)\n",
        "      node_out = self.lin_node(torch.cat((x, aggr), dim=-1))\n",
        "      edge_out = edge_feature + edge_out\n",
        "      node_out = x + node_out\n",
        "      return node_out, edge_out\n",
        "\n",
        "  def message(self, x_i, x_j, edge_feature):\n",
        "    input = torch.cat((x_i, x_j, edge_feature), dim = -1)\n",
        "    output = self.lin_edge(input)\n",
        "    return output\n",
        "\n",
        "  def aggregate(self, inputs, index, dim_size = None):\n",
        "    logging.info(f'aggregate inputs shape, {inputs.shape}') # n_edges, n_edge features  <-128\n",
        "    logging.info(f'aggregate index {index}') # n_edges\n",
        "    logging.info(f'aggregate index {index.shape}')\n",
        "    out = torch_scatter.scatter(inputs, index, dim=self.node_dim, dim_size=dim_size, reduce=\"sum\") # this I need to understand still\n",
        "    return (inputs, out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkwTHqFhLRYI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LearnedSimulator(nn.Module):\n",
        "  def __init__(self,\n",
        "               n_particle_types,\n",
        "               embedding_dim,\n",
        "               hidden_dimensions,\n",
        "               n_layers = 3,\n",
        "               n_mp_layers = 3,\n",
        "               window_size = 6,\n",
        "               dim = 2\n",
        "               ):\n",
        "    super().__init__()\n",
        "    self.type_embeds = nn.Embedding(n_particle_types, embedding_dim)\n",
        "    self.node_preprocess = MLP(embedding_dim + dim * (window_size -1 + 2), hidden_dimensions, hidden_dimensions, n_layers)\n",
        "    self.edge_preprocess = MLP(dim+1, hidden_dimensions, hidden_dimensions, n_layers)\n",
        "    self.node_postprocess = MLP(hidden_dimensions, hidden_dimensions, dim, n_layers)\n",
        "    self.n_mp_layers = n_mp_layers\n",
        "    self.layers = torch.nn.ModuleList()\n",
        "    for _ in range (self.n_mp_layers):\n",
        "      self.layers.append(Processor(hidden_dimensions, hidden_dimensions))\n",
        "\n",
        "  ## Reminder: graph.x -> size = (n_nodes), graph.pos -> size = ((n_nodes,14)), graph.\n",
        "\n",
        "  def forward(self, graph):\n",
        "    type_embedded = self.type_embeds(graph.x)\n",
        "    node_inputs= torch.cat((type_embedded, graph.node_features), dim = -1)\n",
        "    logging.info(f'Shape of node input, {node_inputs.shape}')\n",
        "    node_processed = self.node_preprocess(node_inputs)\n",
        "    edge_processed = self.edge_preprocess(graph.edge_features)\n",
        "    logging.info(f'node_processed, {node_processed.shape}')\n",
        "    logging.info(f'edge_processed,{edge_processed.shape}')\n",
        "    logging.info(f'index, {graph.edge_index}')\n",
        "    for processor_layer in self.layers:\n",
        "      node_processed, edge_processed = processor_layer(node_processed, graph.edge_index, edge_processed)\n",
        "    node_decoded = self.node_postprocess(node_processed)\n",
        "    return node_decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "h7ZUVLms81ED",
        "outputId": "11a51539-dec5-4e2a-8c67-522d2c19b8ac"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241225_135207-8gcv7vr8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/0mche0-my-orga/GNN_for_physics/runs/8gcv7vr8' target=\"_blank\">fine-smoke-4</a></strong> to <a href='https://wandb.ai/0mche0-my-orga/GNN_for_physics' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/0mche0-my-orga/GNN_for_physics' target=\"_blank\">https://wandb.ai/0mche0-my-orga/GNN_for_physics</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/0mche0-my-orga/GNN_for_physics/runs/8gcv7vr8' target=\"_blank\">https://wandb.ai/0mche0-my-orga/GNN_for_physics/runs/8gcv7vr8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/0mche0-my-orga/GNN_for_physics/runs/8gcv7vr8?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7a17e674ecb0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "simulator = LearnedSimulator(9, METADATA['embedding_dimension'],METADATA['hidden_dimension'])\n",
        "\n",
        "wandb.init(project=\"GNN_for_physics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KmzJLAcB9YXe"
      },
      "outputs": [],
      "source": [
        "## Try forwarding the data through the simulator once:\n",
        "logging.info(f'{graph.x}')\n",
        "out=simulator(graph)\n",
        "logging.info(f'this is out shape: {out.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNcWtxoMERgS"
      },
      "source": [
        "# 3. Perform Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "5Vh8itS2EStr",
        "outputId": "1e242fda-f6da-48b9-d33d-8069302710aa"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-357eb4f618fc>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msimulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mDataLoader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfirst_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_short_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMETADATA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1048\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \"\"\"\n\u001b[0;32m-> 1050\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mipu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "# Define the DataLoader\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "simulator = simulator.cuda()\n",
        "DataLoader = pyg.data.DataLoader\n",
        "first_dataloader = DataLoader(dataset = valid_short_dataset, batch_size = METADATA['batch_size'], shuffle = False)\n",
        "\n",
        "valid_dataloader= DataLoader(dataset = valid_short_dataset, batch_size = METADATA['batch_size'])\n",
        "test_dataloader= DataLoader(dataset = test_short_dataset, batch_size = METADATA['batch_size'], shuffle = False)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(params = simulator.parameters(),lr = METADATA['learning_rate'])\n",
        "\n",
        "for epoch in range(METADATA['epochs']):\n",
        "  simulator.train()\n",
        "  progress_bar = tqdm(first_dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\", total=len(first_dataloader))\n",
        "  for i, data in enumerate(progress_bar):\n",
        "    if i == 100:\n",
        "      break\n",
        "    optimizer.zero_grad()\n",
        "    n_particles = len(data.x)\n",
        "    if n_particles >2000:\n",
        "      continue\n",
        "    data = data.cuda()\n",
        "    out = simulator(data)\n",
        "    print(out)\n",
        "    print(data.y)\n",
        "    loss = loss_function(out, data.y)\n",
        "    memory_allocated = torch.cuda.memory_allocated(device=None)\n",
        "    memory_reserved = torch.cuda.memory_reserved(device=None)\n",
        "\n",
        "    if i%100 == 0:\n",
        "      simulator.eval()\n",
        "      losses = []\n",
        "      for i, data in enumerate(test_dataloader):\n",
        "        print('I, DATA', i, data)\n",
        "        break\n",
        "        if i == 25:\n",
        "          break\n",
        "        n_particles = len(data.x)\n",
        "        if n_particles >2000:\n",
        "          continue\n",
        "        data = data.cuda()\n",
        "        out = simulator(data)\n",
        "        loss = loss_function(out, data.y)\n",
        "        losses.append(loss.item())\n",
        "      avg_loss = sum(losses)/len(losses)\n",
        "      simulator.train()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    progress_bar.set_postfix({\"loss\": loss.item(),'n_particles': n_particles, \"memory allocated\": memory_allocated, \"memory reserved\": memory_reserved, \"test avg loss\": avg_loss})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#testing the test dataloader\n",
        "print(len(test_short_dataset))\n"
      ],
      "metadata": {
        "id": "_GZJvb2M8uUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBTM3nYDXc0s"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "del simulator, first_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM0hmqdWby8a"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()  # Collect unreferenced objects\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-SORtGlOVZa"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AA-V75jOWFM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}